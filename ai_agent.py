# -*- coding: utf-8 -*-
"""AI Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ax4OgX1kvmkZqbZxIRUuFcrHtavEtyHs
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade \
import os
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_tavily import TavilySearch
from langchain.agents import AgentExecutor, create_tool_calling_agent, tool
from pinecone import Pinecone
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone.vectorstores import PineconeVectorStore
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.tools import tool


# Set LangSmith variables
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_TRACING_V2"] = "true"
os.environ["LANGSMITH_ENDPOINT"] = "https://eu.api.smith.langchain.com"

os.environ["LANGSMITH_API_KEY"] = st.secrets["LANGSMITH_API_KEY"]
os.environ["LANGSMITH_PROJECT"] = "pr-new-pay-71"

# Set Tavily (Web Search Tool) variables
os.environ["TAVILY_API_KEY"] = st.secrets["TAVILY_API_KEY"]

# Set OpenAI variables
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
model = ChatOpenAI(model="gpt-4o-mini")

#Set Tavily as a tool
search = TavilySearch(max_results=2,
                       description=
        "Verwende das Tool um allgemeine Informationen aus dem Internet zu suchen."
)
tools = [search]

# Setup Pinecone
os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)

# Setup Pinecone Index
from pinecone import ServerlessSpec
index_name = "langchain"
index = pc.Index(index_name)
# index.delete_namespace(namespace="__default__")

#Setup embedding
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")


# from google.colab import drive

# if not os.path.exists("/content/drive/MyDrive"):
#     drive.mount("/content/drive")
#     print("Google Drive wurde gemountet.")
# else:
#     print("Google Drive ist bereits gemountet.")

# auth.authenticate_user()

# from langchain_community.document_loaders import PyPDFLoader

# file_path = "/content/drive/MyDrive/AIAgent/leistungsuebersicht-kfz.pdf"
# loader = PyPDFLoader(file_path)
# docs = loader.load()

#import pprint
#pprint.pp(docs[0].page_content)
#pprint.pp(docs[0].metadata)


#credentials_path = "/content/drive/MyDrive/Credentials/oauth_credentials.json"
#gdrive_api_file = credentials_path
#loader = GoogleDriveLoader(folder_id="1FGAIWQuIs_n13CEyuk_P9yH30UvPHFDS", credentials_path=credentials_path,use_browser=False)
#docs = loader.load()

# Load and split webdocuments
#loader = WebBaseLoader("https://en.wikipedia.org/wiki/Large_language_model")
#document = loader.load()

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# documents = text_splitter.split_documents(docs)

#Setup vector_store
vector_store = PineconeVectorStore(index=index, embedding=embeddings)



# Add documents to Pinecone index
# vector_store.add_documents(documents)


# Create a prompt template for the retrieval chain
retrieval_prompt = ChatPromptTemplate.from_template(
    ("""You are a helpful assistant. Answer the following question based only on the provided context:

    <context>
    {context}
    </context>

    Question: {input}

    Antwort:
    """
    )
)

#Create document chain
document_chain = create_stuff_documents_chain(model, retrieval_prompt)
retriever = vector_store.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)


#Create Retrieval Tool
@tool
def retrieve_document(query:str) -> str:
  """Verwende das Tool um relevante Informationen zu KFZ Versicherung der BGV Versicherung zu suchen"""
  result = retrieval_chain.invoke({"input": query})
  return result["answer"]


tools.append(retrieve_document)

# Create a prompt template for the agent
agent_prompt = ChatPromptTemplate.from_template(
    ("""You are a helpful assistant. Answer the following question:

    Question: {input}

    {agent_scratchpad}

    """
    )
)
#Create Agent
agent = create_tool_calling_agent(model, tools, agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
# result = agent_executor.invoke({
#     "input": "Ist Grobe FahrlÃ¤ssigkeit mitversichert"
# })
# print(result)

#result = agent_executor.invoke({
#    "input": "Wann ist Hulk Hogan gestorben"
#})
#print(result)

def get_rag_response(query: str) -> str:
    result = agent_executor.invoke({
        "input": query
    })
    return str(result)